{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madinak/image_captioning/blob/main/image_captioning_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XkTo__YZ6sA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE6X_otzaE9L"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfrWzNjsp_--"
      },
      "source": [
        "## Download and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxQZDANCaE53",
        "outputId": "6079f693-81c2-43e2-9f2a-036dd01fd2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 804s 0us/step\n",
            "13510582272/13510573713 [==============================] - 804s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Download training image files\n",
        "train_image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + train_image_folder):\n",
        "    train_image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract=True)\n",
        "    train_path = os.path.dirname(train_image_zip) + train_image_folder\n",
        "    os.remove(train_image_zip)\n",
        "else:\n",
        "    train_path = os.path.abspath('.') + train_image_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luBawbIRcFis",
        "outputId": "b2413c0a-c696-4e4f-868c-55f77f426591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/zips/val2014.zip\n",
            "6645014528/6645013297 [==============================] - 389s 0us/step\n",
            "6645022720/6645013297 [==============================] - 389s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Download validation image files\n",
        "val_image_folder = '/val2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + val_image_folder):\n",
        "    val_image_zip = tf.keras.utils.get_file('val2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='http://images.cocodataset.org/zips/val2014.zip',\n",
        "                                      extract=True)\n",
        "    val_path = os.path.dirname(val_image_zip) + val_image_folder\n",
        "    os.remove(val_image_zip)\n",
        "else:\n",
        "    val_path = os.path.abspath('.') + val_image_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKwB5R-aaE3P",
        "outputId": "c5ce6c86-4f47-43cd-e812-34292f48432b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 16s 0us/step\n",
            "252887040/252872794 [==============================] - 16s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "    annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                           extract=True)\n",
        "    train_annot = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "    val_annot = os.path.dirname(annotation_zip)+'/annotations/captions_val2014.json'\n",
        "    os.remove(annotation_zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVuBnL0kaE0r"
      },
      "outputs": [],
      "source": [
        "train_path = 'train2014/'\n",
        "\n",
        "val_path = 'val2014/'\n",
        "\n",
        "train_annot_path = 'annotations/captions_train2014.json'\n",
        "val_annot_path = 'annotations/captions_val2014.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJPPyhnXaEzE"
      },
      "outputs": [],
      "source": [
        "with open(train_annot_path, 'r') as f:\n",
        "    train_annot = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SZpg9gXzi39"
      },
      "outputs": [],
      "source": [
        "with open(val_annot_path, 'r') as f:\n",
        "    val_annot = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzMa2izbaEvU"
      },
      "outputs": [],
      "source": [
        "# train images\n",
        "train_images = train_annot['images']\n",
        "train_captions = train_annot['annotations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gdlhX2faEsl"
      },
      "outputs": [],
      "source": [
        "# group train annotations by image\n",
        "train_dict = collections.defaultdict(list)\n",
        "for x in train_captions:\n",
        "    caption = f\"<start> {x['caption']} <end>\"\n",
        "    image_path = train_path + 'COCO_train2014_' + '%012d.jpg' % (x['image_id'])\n",
        "    train_dict[image_path].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKN3r5NLzeOZ"
      },
      "outputs": [],
      "source": [
        "# validation\n",
        "val_images = val_annot['images']\n",
        "val_captions = val_annot['annotations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBHg6IJmzmZe"
      },
      "outputs": [],
      "source": [
        "# group all val annotations by image\n",
        "val_dict = collections.defaultdict(list)\n",
        "for x in val_captions:\n",
        "    \n",
        "    caption = f\"<start> {x['caption']} <end>\"\n",
        "    val_image_path = val_path + 'COCO_val2014_' + '%012d.jpg' % (x['image_id'])\n",
        "    val_dict[val_image_path].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vSw9Ev9aEm1"
      },
      "outputs": [],
      "source": [
        "def filter_dict(dict_):\n",
        "    new_dict = dict()\n",
        "    # Iterate over all the items in dictionary\n",
        "    for key, value in dict_.items():\n",
        "      if len(value) == 5:\n",
        "        new_dict[key] = value\n",
        "    return new_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D8lS5XZaEkG",
        "outputId": "b2862071-9cbe-4c72-e160-8f62881e0a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed captions lengths 82783 -> 82586\n"
          ]
        }
      ],
      "source": [
        "final_train_dict = filter_dict(train_dict)\n",
        "print('preprocessed captions lengths %d -> %d' % (len(train_dict.keys()), len(final_train_dict.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGVRvD2m0LB0",
        "outputId": "b32db718-4a98-4494-9bb4-ad23296b7061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessed captions lengths 40504 -> 40373\n"
          ]
        }
      ],
      "source": [
        "final_val_dict = filter_dict(val_dict)\n",
        "print('preprocessed captions lengths %d -> %d' % (len(val_dict.keys()), len(final_val_dict.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQM62H6SaEhU",
        "outputId": "d0e4395b-f91b-4c7c-a94b-89b2bbcbf79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82586\n",
            "40373\n"
          ]
        }
      ],
      "source": [
        "print(len(list(final_train_dict.keys())))\n",
        "print(len(list(final_val_dict.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VO9ZDv40iyH"
      },
      "outputs": [],
      "source": [
        "train_keys = list(final_train_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmOhzVPOzmKn",
        "outputId": "ccd5f20b-e8b4-40a2-8b29-242b3f10cb1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ],
      "source": [
        "valid_image_keys = list(final_val_dict.keys())\n",
        "random.shuffle(valid_image_keys)\n",
        "\n",
        "# Select the first 5000 image_paths for validation\n",
        "val_keys = valid_image_keys[:5000]\n",
        "print(len(val_keys))\n",
        "\n",
        "# Select the last 5000 image_paths for test\n",
        "test_keys = valid_image_keys[-5000:]\n",
        "print(len(test_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh88dgDEaEZT"
      },
      "outputs": [],
      "source": [
        "# save as json for future use\n",
        "with open('train_keys.json', 'w') as f:\n",
        "    json.dump(train_keys, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3UEIC8haEW7"
      },
      "outputs": [],
      "source": [
        "with open('val_keys.json', 'w') as f:\n",
        "    json.dump(val_keys, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsKztDjGROFq"
      },
      "outputs": [],
      "source": [
        "with open('test_keys.json', 'w') as f:\n",
        "    json.dump(test_keys, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peJnnbAUaEUI"
      },
      "outputs": [],
      "source": [
        "# open train_keys json \n",
        "with open('train_keys.json', 'r') as f:\n",
        "    train_keys = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-B_ZF-aaERT"
      },
      "outputs": [],
      "source": [
        "# open val_keys json \n",
        "with open('val_keys.json', 'r') as f:\n",
        "    val_keys = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open test_keys json \n",
        "with open('test_keys.json', 'r') as f:\n",
        "    test_keys = json.load(f)"
      ],
      "metadata": {
        "id": "3CQHcXsWiiSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R916H6Rdp-N7"
      },
      "outputs": [],
      "source": [
        "def subset(data_dict, image_names):\n",
        "    subset_dict = {image_name:captions for image_name, captions in data_dict.items() if image_name in image_names}\n",
        "    return subset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcLuvCe1nv9U"
      },
      "outputs": [],
      "source": [
        "train_data = dict(final_train_dict)\n",
        "valid_data = subset(final_val_dict, val_keys)\n",
        "test_data = subset(final_val_dict, test_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSuAKAnHWR",
        "outputId": "9dc934d5-8054-4ae9-b08a-7ec5ef7195b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  82586\n",
            "Number of validation samples:  5000\n",
            "Number of testing samples:  5000\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))\n",
        "print(\"Number of testing samples: \", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_data.json', 'w') as f:\n",
        "    json.dump(train_data, f)"
      ],
      "metadata": {
        "id": "p6SZwDzJ-EGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('valid_data.json', 'w') as f:\n",
        "    json.dump(valid_data, f)"
      ],
      "metadata": {
        "id": "KOvULZRZ-D9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test_data.json', 'w') as f:\n",
        "    json.dump(test_data, f)"
      ],
      "metadata": {
        "id": "ZZwy67AD-D1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open train_data json \n",
        "with open('train_data.json', 'r') as f:\n",
        "    train_data = json.load(f)"
      ],
      "metadata": {
        "id": "U7I_vYiQ-7wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open valid_data json \n",
        "with open('valid_data.json', 'r') as f:\n",
        "    valid_data = json.load(f)"
      ],
      "metadata": {
        "id": "W4z9AIMm-7oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open test_data json \n",
        "with open('test_data.json', 'r') as f:\n",
        "    test_data = json.load(f)"
      ],
      "metadata": {
        "id": "9v1vrhjB-7fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhDf9lIX2rzE"
      },
      "source": [
        "## Pre-process captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYdalVcrItY9"
      },
      "outputs": [],
      "source": [
        "train_paths = list(train_data.keys())\n",
        "val_paths = list(valid_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8U5UTt3ItWY"
      },
      "outputs": [],
      "source": [
        "train_c = []\n",
        "train_i = []\n",
        "\n",
        "for i in train_paths:\n",
        "  train_caption_list = train_data[i]\n",
        "  train_c.extend(train_caption_list)\n",
        "  train_i.extend([i] * len(train_caption_list))\n",
        "\n",
        "val_c = []\n",
        "val_i = []\n",
        "\n",
        "for i in val_paths:\n",
        "  val_caption_list = valid_data[i]\n",
        "  val_c.extend(val_caption_list)\n",
        "  val_i.extend([i] * len(val_caption_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWTWDwtal5dk"
      },
      "outputs": [],
      "source": [
        "all_c = train_c + val_c\n",
        "all_i = train_i + val_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1PXD12lrmLQ",
        "outputId": "95a8cf29-5468-4545-d1c2-3b90163795f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "# with Tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, \n",
        "                                                  oov_token=\"<unk>\", \n",
        "                                                  lower=True,\n",
        "                                                  split=' ',\n",
        "                                                  filters='!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~')\n",
        "tokenizer.fit_on_texts(all_c)\n",
        "seqs = tokenizer.texts_to_sequences(all_c)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(seqs, padding='post')\n",
        "\n",
        "# Find the maximum length of any caption\n",
        "def max_caption_length(captions):\n",
        "    return max(len(caption) for caption in captions)\n",
        "\n",
        "# Calculate the max_length\n",
        "max_length = max_caption_length(seqs)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Z2pfdJfy15EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18z8s0S-ItQ1"
      },
      "outputs": [],
      "source": [
        "merge_dict = collections.defaultdict(list)\n",
        "for image, caption in zip(all_i, cap_vector):\n",
        "    merge_dict[image].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzIrQsx5ItNw",
        "outputId": "73ebc5ea-01eb-472b-fd23-13584d4dc8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train length: 82586\n",
            "val length: 5000\n"
          ]
        }
      ],
      "source": [
        "train_slice = []\n",
        "val_slice = []\n",
        "\n",
        "for i in all_i:\n",
        "  if i[:i.index(\"/\")] == 'train2014':\n",
        "    train_slice.append(i)\n",
        "  else:\n",
        "    val_slice.append(i)\n",
        "\n",
        "train_slice = set(train_slice)\n",
        "val_slice = set(val_slice)\n",
        "print('train length:', len(train_slice))\n",
        "print('val length:', len(val_slice))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byl2gK5qoRTx",
        "outputId": "59189000-641e-4008-ac27-c0c20eabbc15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(412930, 412930, 25000, 25000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_images = []\n",
        "train_captions = []\n",
        "for imgt in train_slice:\n",
        "    capt_len = len(merge_dict[imgt])\n",
        "    train_images.extend([imgt] * capt_len)\n",
        "    train_captions.extend(merge_dict[imgt])\n",
        "\n",
        "val_images = []\n",
        "val_captions = []\n",
        "for imgv in val_slice:\n",
        "    capv_len = len(merge_dict[imgv])\n",
        "    val_images.extend([imgv] * capv_len)\n",
        "    val_captions.extend(merge_dict[imgv])\n",
        "\n",
        "len(train_images), len(train_captions), len(val_images), len(val_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y-S9sTdikMt"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare tf.data"
      ],
      "metadata": {
        "id": "E8bC3HfhylBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDY6EIDtiiC-"
      },
      "outputs": [],
      "source": [
        "def load_image_captions(image_path, captions, size=(224, 224)):  \n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, size)\n",
        "    image = preprocess_input(image) \n",
        "    return image, captions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000"
      ],
      "metadata": {
        "id": "Gd6MpWFl2Hju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3OGudXis1BW"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_captions))\n",
        "train_dataset = train_dataset.map(load_image_captions, \n",
        "                       num_parallel_calls=tf.data.experimental.AUTOTUNE) \n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR8cDrIYtdes"
      },
      "outputs": [],
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_captions))\n",
        "val_dataset = val_dataset.map(load_image_captions, \n",
        "                       num_parallel_calls=tf.data.experimental.AUTOTUNE) \n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "F0FgFy5Ry8I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "num_layers = 4\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "vocabulary_size = tokenizer.num_words + 1\n",
        "dropout_rate = 0.3"
      ],
      "metadata": {
        "id": "gFrmqkrP0GE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz5HcdgFihw7"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUgWQTkWtv0J"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiIcUpmFtyzS"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits,\n",
        "        axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML5mxEr-t30w"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGYpdmwPt7ns"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAafSsUvt-Xg"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.3):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # (batch_size, input_seq_len, d_model)\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # (batch_size, input_seq_len, d_model)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # (batch_size, input_seq_len, d_model)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
        "\n",
        "        return out2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwTSognmuDyH"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                   maximum_position_encoding=49, dropout_rate=0.3):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        rs50 = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "        \n",
        "        self.features_extract = rs50\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(d_model, activation='relu')\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "                \n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) \n",
        "                           for _ in range(num_layers)]\n",
        "    \n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def train_resnet(self, trainable=False):\n",
        "        for layer in self.features_extract.layers:\n",
        "                layer.trainable = trainable\n",
        "                \n",
        "        \n",
        "    def call(self, x, training, mask=None):\n",
        "        x = self.features_extract(x)\n",
        "        x = tf.reshape(x,\n",
        "                    (x.shape[0], -1, x.shape[3]))\n",
        "\n",
        "        seq_len = tf.shape(x)[1] \n",
        "        x = self.fc(x) \n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask=None)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHFbsaHxuMWM"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.3):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(\n",
        "            x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x) \n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W90rzb4IuS_k"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "                 maximum_position_encoding, rate=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(\n",
        "                                maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                   look_ahead_mask, \n",
        "                                                   padding_mask)\n",
        "\n",
        "        attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
        "        attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1WrAT9KuVxS"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "             vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                                     pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                             vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "         look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "          tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9K0Xpdfujtv"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff, vocab_size=vocabulary_size, \n",
        "                          pe_input=49, pe_target=vocabulary_size, rate=dropout_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEzB-QKmvymX"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp, tar):\n",
        "    inp_seq = tf.ones([inp.shape[0], 49])\n",
        "    enc_padding_mask = create_padding_mask(inp_seq)\n",
        "    dec_padding_mask = create_padding_mask(inp_seq)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUCZNYYVulP2"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL8gpsMDvsHS"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='val_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWP31wuavsHS"
      },
      "outputs": [],
      "source": [
        "train_loss_plot = []\n",
        "train_acc_plot = []\n",
        "\n",
        "val_loss_plot = []\n",
        "val_acc_plot = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.makedirs(os.path.join('checkpoints', 'trans'))\n",
        "except FileExistsError:\n",
        "    pass # if directory already exists\n",
        "checkpoint_path = \"./checkpoints/trans\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "56cSswgcAjSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "cDxv1cDnwFyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb_-ISQmvsHS"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, \n",
        "                                     combined_mask, dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8exZMJJvsHT"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(val_inp, val_tar):\n",
        "    val_tar_inp = val_tar[:, :-1]\n",
        "    val_tar_real = val_tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(val_inp, val_tar_inp)\n",
        "    val_predictions, _ = transformer(val_inp, val_tar_inp, False, enc_padding_mask,\n",
        "                                     combined_mask, dec_padding_mask)\n",
        "    v_loss = loss_function(val_tar_real, val_predictions)\n",
        "\n",
        "    val_loss(v_loss)\n",
        "    val_accuracy(val_tar_real, val_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1-O1QzpvsHT"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "patience = 5\n",
        "wait = 0\n",
        "best = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for batch, (inp, tar) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        # Log every 1000 batches.\n",
        "        if batch % 1000 == 0:\n",
        "            print(\"Training loss (for one batch) at batch %d: %.4f\" % (batch, float(train_loss.result())))\n",
        "            print(\"Training acc (for one batch) at batch %d: %.4f\" % (batch, float(train_accuracy.result())))\n",
        "\n",
        "    \n",
        "    # Display metrics at the end of each epoch.\n",
        "    #train_loss = train_loss.result()\n",
        "    print(\"Training loss over epoch: %.4f\" % (float(train_loss.result()),))\n",
        "\n",
        "    #train_acc = train_accuracy.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_accuracy.result()),))\n",
        "\n",
        "    train_loss_plot.append(train_loss.result())\n",
        "    train_acc_plot.append(train_accuracy.result())\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for val_inp, val_tar in val_dataset:\n",
        "        test_step(val_inp, val_tar)\n",
        "\n",
        "    #val_loss = val_loss.result()\n",
        "    #val_acc = val_accuracy.result()\n",
        "    \n",
        "    print(\"Validation loss: %.4f\" % (float(val_loss.result()),))\n",
        "    print(\"Validation acc: %.4f\" % (float(val_accuracy.result()),))\n",
        "\n",
        "    val_loss_plot.append(val_loss.result())\n",
        "    val_acc_plot.append(val_accuracy.result())\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint_path = ckpt_manager.save()\n",
        "      print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')\n",
        "\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "    val_loss.reset_states()\n",
        "    val_accuracy.reset_states()\n",
        "\n",
        "    # early stopping \n",
        "    wait += 1\n",
        "    if val_loss > best:\n",
        "      best = val_loss\n",
        "      wait = 0\n",
        "    if wait >= patience:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_plot)\n",
        "plt.plot(val_loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "aAomHmNAvZGe",
        "outputId": "c66334bd-925c-4b9d-edcc-d508c1f01af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXXc_VSeKha0"
      },
      "outputs": [],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore from checkpoints\n",
        "\n",
        "checkpoint_path = \"./checkpoints/trans\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, \n",
        "                                          max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "metadata": {
        "id": "dKmVVUfgFG-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "_YkAcMynwKwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg-kH96td3cW"
      },
      "outputs": [],
      "source": [
        "def evaluate(image, max_length=21):\n",
        "   image = tf.io.read_file(image)\n",
        "   image = tf.io.decode_jpeg(image, channels=3)\n",
        "   image = tf.image.resize(image, size=(224, 224))\n",
        "   image = tf.expand_dims(image, axis=0)\n",
        "   image = preprocess_input(image)\n",
        "   \n",
        "   start_token = tokenizer.word_index['<start>']\n",
        "   end_token = tokenizer.word_index['<end>']\n",
        "   decoder_input = [start_token]\n",
        "   output = tf.expand_dims(decoder_input, 0)\n",
        "   result = []\n",
        "\n",
        "   for i in range(max_length):\n",
        "      enc_padding_mask, combined_mask, dec_padding_mask = create_masks(image, output)\n",
        "      predictions, attention_weights = transformer(image, output, False, enc_padding_mask,\n",
        "                                     combined_mask, dec_padding_mask)\n",
        "      predictions = predictions[: ,-1:, :]\n",
        "      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "      if predicted_id == end_token:\n",
        "         return result,tf.squeeze(output, axis=0), attention_weights\n",
        "      result.append(tokenizer.index_word[int(predicted_id)])\n",
        "      output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "   return result, tf.squeeze(output, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean test captions\n",
        "for key, captions in test_data.items():   \n",
        "    for i, caption in enumerate (captions):\n",
        "        caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
        "        clean_words = [word for word in caption_nopunct.split()]\n",
        "        clean_words = clean_words[1:-1]\n",
        "        caption_new = ' '.join(clean_words)\n",
        "        captions[i] = caption_new"
      ],
      "metadata": {
        "id": "ckCh-zjxj0J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize\n",
        "image = np.random.choice(test_keys)\n",
        "caption, result, attention_weights = evaluate(image)\n",
        "res_join = ' '.join(caption)\n",
        "gt = test_data[image]\n",
        "\n",
        "img = mpimg.imread(image)\n",
        "h, w, _= np.shape(img)\n",
        "img_plate=np.zeros((h, w + 150, 3), dtype=np.uint8) + 255\n",
        "img_plate[0:h, 0:w, :] = img\n",
        "\n",
        "dpi = 50\n",
        "figsize = 0.8 * w / float(dpi), 0.8 * h / float(dpi)\n",
        "\n",
        "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.text(w + 20, 40, 'Ground truth:', fontsize=20)\n",
        "ax.text(w + 20, 210, '\\n'.join(gt), fontsize=18)\n",
        "\n",
        "ax.text(w + 20, 270, 'Transformer:', fontsize=20)\n",
        "ax.text(w + 20, 310, res_join, fontsize=18)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(img_plate)"
      ],
      "metadata": {
        "id": "VmsDWPwE7MPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test evaluation"
      ],
      "metadata": {
        "id": "v416TKXcwP4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_captions(test_images, max_length=16):\n",
        "    captions = []\n",
        "    for i, image in tqdm(enumerate(test_images)):\n",
        "        captions.append(evaluate(image, max_length)[0])\n",
        "\n",
        "    return captions"
      ],
      "metadata": {
        "id": "S-WzviuQ1v_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_captions = generate_test_captions(test_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgTrH-PqoWUf",
        "outputId": "3bedc09f-a417-44f0-a26a-a4918a720daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [2:44:04,  1.97s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_captions[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm3vPaYP81Sk",
        "outputId": "0fe6b698-d4f2-43cc-b611-3f2bea407600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a', 'vase', 'with', 'flowers', 'sitting', 'on', 'a', 'table'],\n",
              " ['a', 'man', 'riding', 'a', 'horse', 'in', 'front', 'of', 'a', 'building']]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join_cap = [' '.join(i) for i in generated_captions]\n",
        "join_cap[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2no3C75q-Oe4",
        "outputId": "63c1c978-286c-4884-9f35-81195c49c65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a vase with flowers sitting on a table',\n",
              " 'a man riding a horse in front of a building']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_captions = []\n",
        "\n",
        "for i, j in zip(test_keys, join_cap):\n",
        "    pred_captions.append({\"image_id\": int(i[27:33]), \"caption\": j})"
      ],
      "metadata": {
        "id": "mSphqMVy-m2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pred_captions_trans.json', 'w') as fp:\n",
        "    json.dump(pred_captions, fp)"
      ],
      "metadata": {
        "id": "xZqSkA7i--Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_captions[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acfs1w3y-mwH",
        "outputId": "07f98beb-f62a-43ac-c0e0-8385a3cea16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'caption': 'a vase with flowers sitting on a table', 'image_id': 248085},\n",
              " {'caption': 'a man riding a horse in front of a building',\n",
              "  'image_id': 376208}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycocoevalcap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jV-jZM-msZ",
        "outputId": "7de7dcfb-c957-4b84-e592-a429e3dff913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 104.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from pycocoevalcap) (2.0.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.15.0)\n",
            "Installing collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "import json\n",
        "from json import encoder\n",
        "encoder.FLOAT_REPR = lambda o: format(o, '.3f')\n",
        "import sys\n",
        "\n",
        "input_json = sys.argv[1]\n",
        "\n",
        "\n",
        "annFile = 'annotations/captions_val2014.json'\n",
        "coco = COCO(annFile)\n",
        "valids = coco.getImgIds()\n",
        "\n",
        "load = json.load(open('pred_captions_trans.json', 'r'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJYdfi2x-4pK",
        "outputId": "7e329c2c-a5ce-48c6-e577-bb8af1a325c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.35s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter results to only those in MSCOCO validation set\n",
        "preds_filt = [p for p in load if p['image_id'] in valids]\n",
        "print('using %d/%d predictions' % (len(preds_filt), len(load)))\n",
        "json.dump(preds_filt, open('tmp.json', 'w'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXH7r3t0-4lb",
        "outputId": "af2e8a19-7fde-4816-d02c-bdbf084cfc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using 5000/5000 predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cocoRes = coco.loadRes(preds_filt)\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
        "cocoEval.evaluate()\n",
        "\n",
        "# create output dictionary\n",
        "out = {}\n",
        "for metric, score in cocoEval.eval.items():\n",
        "    out[metric] = score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVZMP3vd-4iC",
        "outputId": "9672872d-0dec-4056-9680-f41f457dbd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "setting up scorers...\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "computing Bleu score...\n",
            "{'testlen': 48152, 'reflen': 47865, 'guess': [48152, 43152, 38152, 33152], 'correct': [30236, 13369, 5431, 2391]}\n",
            "ratio: 1.0059960305024338\n",
            "Bleu_1: 0.628\n",
            "Bleu_2: 0.441\n",
            "Bleu_3: 0.303\n",
            "Bleu_4: 0.211\n",
            "computing METEOR score...\n",
            "METEOR: 0.203\n",
            "computing Rouge score...\n",
            "ROUGE_L: 0.462\n",
            "computing CIDEr score...\n",
            "CIDEr: 0.647\n",
            "computing SPICE score...\n",
            "SPICE: 0.128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1NJI98QCy6E"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "image_captioning_full.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNP2L/tAbVDPBEuyd1RuQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}